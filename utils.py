import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import io
import hashlib
import time
import json
import pickle
import os
from datetime import datetime, timedelta

# å†…å­˜ç¼“å­˜é…ç½®
MEMORY_CACHE_SIZE = 2000  # å†…å­˜ç¼“å­˜æœ€å¤§æ¡ç›®æ•°
MEMORY_CACHE_TTL_HOURS = 24  # å†…å­˜ç¼“å­˜è¿‡æœŸæ—¶é—´ï¼ˆå°æ—¶ï¼‰

def filter_dataframe(df, filters):
    """æ ¹æ®ç­›é€‰æ¡ä»¶è¿‡æ»¤DataFrame"""
    filtered_df = df.copy()
    
    for filter_type, filter_value in filters.items():
        if filter_value and filter_value != 'å…¨éƒ¨':
            if filter_type == 'Brand':
                filtered_df = filtered_df[filtered_df['Brand'] == filter_value]
            elif filter_type == 'Asin':
                filtered_df = filtered_df[filtered_df['Asin'] == filter_value]
            elif filter_type == 'Rating':
                filtered_df = filtered_df[filtered_df['Rating'] == filter_value]
            elif filter_type == 'Review Type':
                filtered_df = filtered_df[filtered_df['Review Type'] == filter_value]
            elif filter_type == 'row_range':
                start_row, end_row = filter_value
                filtered_df = filtered_df.iloc[start_row:end_row]
    
    return filtered_df

def process_data(df, brand_df=None):
    """æ•°æ®é¢„å¤„ç†å‡½æ•°"""
    # ç¡®ä¿æ‰€éœ€åˆ—å­˜åœ¨
    required_columns = ['Asin', 'Title', 'Content', 'Model', 'Rating', 'Date']
    if not all(col in df.columns for col in required_columns):
        st.error(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {[col for col in required_columns if col not in df.columns]}")
        return None
    
    # åªä¿ç•™å¿…è¦çš„åˆ—
    df = df[required_columns].copy()
    
    # å‘é‡åŒ–æ“ä½œ
    df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')
    df['Date'] = pd.to_datetime(df['Date'], errors='coerce')
    
    # ä½¿ç”¨str.strip()çš„å‘é‡åŒ–æ“ä½œ
    text_columns = ['Title', 'Content', 'Model']
    for col in text_columns:
        df[col] = df[col].astype(str).str.strip()
    
    # æ·»åŠ IDåˆ—ï¼ˆç¡®ä¿å”¯ä¸€æ€§ï¼‰
    df.insert(0, 'ID', range(1, len(df) + 1))
    
    # ä½¿ç”¨å‘é‡åŒ–æ“ä½œæ›¿ä»£apply
    df['Review Type'] = pd.cut(
        df['Rating'],
        bins=[-float('inf'), 2, 3, float('inf')],
        labels=['negative', 'neutral', 'positive']
    )
    
    # å¦‚æœæä¾›äº†å“ç‰Œæ•°æ®ï¼Œè¿›è¡Œå…³è”
    if brand_df is not None and 'ASIN' in brand_df.columns and 'Brand' in brand_df.columns:
        # æ¸…ç†å“ç‰Œæ•°æ®
        brand_columns = ['ASIN', 'Brand']
        if 'Parent ASIN' in brand_df.columns:
            brand_columns.append('Parent ASIN')
        
        brand_df = brand_df[brand_columns].copy()
        brand_df['ASIN'] = brand_df['ASIN'].astype(str).str.strip()
        brand_df['Brand'] = brand_df['Brand'].astype(str).str.strip()
        if 'Parent ASIN' in brand_df.columns:
            brand_df['Parent ASIN'] = brand_df['Parent ASIN'].astype(str).str.strip()
        
        # å¤„ç†é‡å¤çš„ASINï¼Œä¿ç•™æœ€æ–°çš„å“ç‰Œä¿¡æ¯
        brand_df = brand_df.drop_duplicates(subset=['ASIN'], keep='last')
        
        # å¤‡ä»½åŸå§‹ID
        original_ids = df['ID'].copy()
        
        # ç¬¬ä¸€è½®ï¼šé€šè¿‡ASINå…³è”å“ç‰Œä¿¡æ¯
        df = df.merge(brand_df[['ASIN', 'Brand']], left_on='Asin', right_on='ASIN', how='left')
        
        # ç»Ÿè®¡ç¬¬ä¸€è½®åŒ¹é…ç»“æœ
        first_round_matches = df['Brand'].notna().sum()
        
        # ç¬¬äºŒè½®ï¼šå¦‚æœç¬¬ä¸€è½®æ²¡æœ‰åŒ¹é…ä¸Šï¼Œä¸”å­˜åœ¨Parent ASINï¼Œåˆ™ç”¨Parent ASINè¿æ¥
        if 'Parent ASIN' in brand_df.columns and first_round_matches < len(df):
            # æ‰¾å‡ºç¬¬ä¸€è½®æ²¡æœ‰åŒ¹é…ä¸Šçš„è®°å½•
            unmatched_mask = df['Brand'].isna()
            unmatched_df = df[unmatched_mask].copy()
            
            if len(unmatched_df) > 0:
                # åˆ›å»ºParent ASINè¿æ¥ç”¨çš„å“ç‰Œæ•°æ®
                parent_brand_df = brand_df[brand_df['Parent ASIN'].notna()].copy()
                parent_brand_df = parent_brand_df[['Parent ASIN', 'Brand']].rename(columns={'Parent ASIN': 'ASIN'})
                
                # é€šè¿‡Parent ASINè¿›è¡Œç¬¬äºŒè½®è¿æ¥
                unmatched_df = unmatched_df.merge(parent_brand_df, left_on='Asin', right_on='ASIN', how='left', suffixes=('', '_parent'))
                
                # æ›´æ–°åŸæ•°æ®ä¸­æœªåŒ¹é…çš„è®°å½•
                df.loc[unmatched_mask, 'Brand'] = unmatched_df['Brand']
                
                # æ¸…ç†å¤šä½™çš„åˆ—
                if 'ASIN_y' in df.columns:
                    df = df.drop(columns=['ASIN_y'])
                if 'ASIN_parent' in df.columns:
                    df = df.drop(columns=['ASIN_parent'])
        
        # æ¢å¤åŸå§‹ID
        df['ID'] = original_ids
        
        # ç»Ÿè®¡æœ€ç»ˆåŒ¹é…ç»“æœ
        final_matches = df['Brand'].notna().sum()
        total_records = len(df)
        match_rate = (final_matches / total_records * 100) if total_records > 0 else 0
        
        st.success(f"âœ… æˆåŠŸå…³è”å“ç‰Œæ•°æ®ï¼")
        st.info(f"ğŸ“Š åŒ¹é…ç»Ÿè®¡ï¼š")
        st.info(f"   - æ€»è®°å½•æ•°ï¼š{total_records}")
        st.info(f"   - æˆåŠŸåŒ¹é…ï¼š{final_matches}")
        st.info(f"   - åŒ¹é…ç‡ï¼š{match_rate:.1f}%")
        
        if 'Parent ASIN' in brand_df.columns:
            st.info(f"   - ç¬¬ä¸€è½®ASINåŒ¹é…ï¼š{first_round_matches}")
            st.info(f"   - ç¬¬äºŒè½®Parent ASINåŒ¹é…ï¼š{final_matches - first_round_matches}")
    
    # é‡æ–°æ’åºåˆ—
    column_order = ['ID', 'Asin', 'Brand', 'Title', 'Content', 'Model', 'Rating', 'Date', 'Review Type']
    existing_columns = [col for col in column_order if col in df.columns]
    df = df[existing_columns]
    
    return df

def calculate_review_stats(df):
    """è®¡ç®—è¯„è®ºç±»å‹çš„ç»Ÿè®¡ä¿¡æ¯"""
    # è®¡ç®—å„ç±»å‹æ•°é‡
    review_counts = df['Review Type'].value_counts()
    
    # è®¡ç®—ç™¾åˆ†æ¯”
    review_percentages = (review_counts / len(df) * 100).round(2)
    
    # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
    stats_df = pd.DataFrame({
        'æ•°é‡': review_counts,
        'å æ¯”(%)': review_percentages
    })
    
    return stats_df, review_counts, review_percentages

def create_pie_chart(review_counts, title='è¯„è®ºç±»å‹åˆ†å¸ƒ'):
    # ç¡®ä¿ç´¢å¼•é¡ºåºä¸é¢œè‰²æ˜ å°„ä¸€è‡´
    review_counts = review_counts.reindex(
        ['positive', 'neutral', 'negative'], 
        fill_value=0
    )
    
    # åˆ›å»ºæ•°æ®æ¡†ç¡®ä¿é¡ºåº
    df = pd.DataFrame({
        'type': review_counts.index,
        'count': review_counts.values
    })
    
    fig = px.pie(
        df,
        values='count',
        names='type',
        title=title,
        color='type',  # å…³é”®ï¼šé€šè¿‡colorå‚æ•°æŒ‡å®šåˆ†ç»„
        color_discrete_map={
            'positive': '#2ECC71',  # ç»¿è‰²
            'neutral': '#F1C40F',   # é»„è‰²
            'negative': '#E74C3C'   # çº¢è‰²
        },
        category_orders={'type': ['positive', 'neutral', 'negative']}
    )
    
    # ç¦ç”¨ä¸»é¢˜å¹²æ‰°
    fig.update_layout(template='none')
    fig.update_traces(
        textposition='inside', 
        textinfo='percent+label',
        marker=dict(line=dict(color='#FFFFFF', width=1))  # æ·»åŠ ç™½è‰²è¾¹æ¡†
    )
    return fig

def analyze_by_group(df, group_by):
    """æŒ‰æŒ‡å®šå­—æ®µè¿›è¡Œåˆ†ç»„åˆ†æ"""
    # ä½¿ç”¨æ›´é«˜æ•ˆçš„åˆ†ç»„æ“ä½œ
    if isinstance(group_by, list):
        if 'Brand' in group_by:
            df['Group'] = df['Brand'] + ' - ' + df['Asin'] + ' - ' + df['Model']
        else:
            df['Group'] = df['Asin'] + ' - ' + df['Model']
        group_by = 'Group'
    
    # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰ç»Ÿè®¡ä¿¡æ¯
    stats = df.groupby(group_by).agg({
        'Rating': ['count', 'mean', 'std'],
        'Review Type': lambda x: x.value_counts().to_dict()
    }).round(2)
    
    # é‡å‘½ååˆ—
    stats.columns = ['è¯„è®ºæ•°é‡', 'å¹³å‡è¯„åˆ†', 'æ ‡å‡†å·®', 'è¯„è®ºç±»å‹åˆ†å¸ƒ']
    
    # è®¡ç®—è¯„åˆ†åˆ†å¸ƒ
    rating_dist = pd.crosstab(df[group_by], df['Rating'], normalize='index') * 100
    
    return stats, rating_dist, group_by

def create_rating_trend_chart(df, group_by):
    """åˆ›å»ºè¯„åˆ†è¶‹åŠ¿å›¾"""
    # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ—¶é—´å¤„ç†
    df['Month'] = df['Date'].dt.to_period('M').astype(str)
    trend_data = df.groupby(['Month', group_by])['Rating'].mean().reset_index()
    
    # åˆ›å»ºè¶‹åŠ¿å›¾
    title = f'{group_by}éšæ—¶é—´çš„å¹³å‡è¯„åˆ†å˜åŒ–'
    
    fig = px.line(trend_data, 
                  x='Month', 
                  y='Rating', 
                  color=group_by,
                  title=title,
                  labels={'Rating': 'å¹³å‡è¯„åˆ†', 'Month': 'æœˆä»½'})
    
    fig.update_xaxes(tickangle=45)
    return fig

def create_rating_heatmap(rating_dist_pct, title):
    """åˆ›å»ºè¯„åˆ†åˆ†å¸ƒçƒ­åŠ›å›¾"""
    fig = go.Figure(data=go.Heatmap(
        z=rating_dist_pct.values,
        x=rating_dist_pct.columns,
        y=rating_dist_pct.index,
        colorscale='RdYlGn',
        text=rating_dist_pct.round(1).values,
        texttemplate='%{text}%',
        textfont={"size": 10},
        hoverongaps=False))
    
    fig.update_layout(
        title=title,
        xaxis_title='è¯„åˆ†',
        yaxis_title='äº§å“',
        height=max(300, len(rating_dist_pct) * 30))
    
    return fig

def create_rating_pie_chart(rating_dist_pct, title):
    """åˆ›å»ºè¯„åˆ†åˆ†å¸ƒé¥¼å›¾"""
    # åˆ›å»ºä¸€ä¸ªç©ºçš„å›¾è¡¨åˆ—è¡¨
    figs = []
    
    # å®šä¹‰é¢œè‰²æ˜ å°„ - ç¡®ä¿æ˜Ÿçº§å’Œé¢œè‰²ä¸€ä¸€å¯¹åº”
    color_map = {
        5: 'rgb(0, 128, 0)',      # ç»¿è‰² - 5æ˜Ÿ
        4: 'rgb(135, 206, 235)',  # æµ…è“è‰² - 4æ˜Ÿ
        3: 'rgb(255, 215, 0)',    # é»„è‰² - 3æ˜Ÿ
        2: 'rgb(255, 0, 0)',      # çº¢è‰² - 2æ˜Ÿ
        1: 'rgb(139, 0, 0)'       # æ·±çº¢è‰² - 1æ˜Ÿ
    }
    
    # ä¸ºæ¯ä¸ªASINæˆ–ASIN+Modelåˆ›å»ºé¥¼å›¾
    for idx, row in rating_dist_pct.iterrows():
        # åˆ›å»ºæ•°æ®æ¡†
        df_pie = pd.DataFrame({
            'è¯„åˆ†': row.index,
            'å æ¯”': row.values
        })
        
        # è¿‡æ»¤æ‰å æ¯”ä¸º0çš„æ•°æ®
        df_pie = df_pie[df_pie['å æ¯”'] > 0]
        
        # ç¡®ä¿è¯„åˆ†æŒ‰é™åºæ’åˆ—
        df_pie = df_pie.sort_values('è¯„åˆ†', ascending=False)
        
        # ä½¿ç”¨go.Figureç›´æ¥åˆ›å»ºé¥¼å›¾
        fig = go.Figure(data=[go.Pie(
            labels=df_pie['è¯„åˆ†'],
            values=df_pie['å æ¯”'],
            hole=0.3,
            marker=dict(
                colors=[color_map[rating] for rating in df_pie['è¯„åˆ†']],
                line=dict(color='#FFFFFF', width=1)
            ),
            textinfo='percent+label',
            textposition='outside',
            textfont_size=14,
            insidetextorientation='horizontal',
            hovertemplate='%{label}: %{percent:.1%}<extra></extra>',
            texttemplate='%{label}<br>%{percent:.1%}'
        )])
        
        # æ›´æ–°å¸ƒå±€
        fig.update_layout(
            title=f'{idx} è¯„åˆ†åˆ†å¸ƒ',
            showlegend=False,  # å»é™¤å›¾ä¾‹
            margin=dict(t=50, b=50, l=50, r=50),  # è°ƒæ•´è¾¹è·
            uniformtext_minsize=12,  # è®¾ç½®æœ€å°æ–‡æœ¬å¤§å°
            uniformtext_mode='hide'  # éšè—å¤ªå°çš„æ–‡æœ¬
        )
        
        # ä¼˜åŒ–æ ‡ç­¾ä½ç½®å’Œæ˜¾ç¤º
        fig.update_traces(
            textposition='outside',
            textinfo='percent+label',
            rotation=0  # ä»0åº¦å¼€å§‹
        )
        
        figs.append(fig)
    
    return figs

def save_fig_to_html(fig, filename):
    """ä¿å­˜å›¾è¡¨ä¸ºHTMLæ–‡ä»¶"""
    return fig.to_html()

def get_download_data(df, file_format='excel'):
    """å‡†å¤‡ä¸‹è½½æ•°æ®"""
    if file_format == 'excel':
        output = io.BytesIO()
        with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
            df.to_excel(writer, index=False)
        data = output.getvalue()
        return data
    else:  # txt format
        # å°†DataFrameè½¬æ¢ä¸ºæ ¼å¼åŒ–çš„æ–‡æœ¬
        output = io.StringIO()
        
        # å†™å…¥è¡¨å¤´
        headers = df.columns.tolist()
        output.write('\t'.join(headers) + '\n')
        output.write('-' * 100 + '\n')  # åˆ†éš”çº¿
        
        # å†™å…¥æ•°æ®è¡Œ
        for _, row in df.iterrows():
            # ç¡®ä¿æ‰€æœ‰å€¼éƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶å¤„ç†å¯èƒ½çš„ç©ºå€¼
            row_values = [str(val) if pd.notna(val) else '' for val in row]
            # ä½¿ç”¨åˆ¶è¡¨ç¬¦åˆ†éš”ï¼Œè¿™æ ·åœ¨æ–‡æœ¬ç¼–è¾‘å™¨ä¸­ä¼šå¯¹é½
            output.write('\t'.join(row_values) + '\n')
        
        return output.getvalue().encode('utf-8')

# è…¾è®¯ç¿»è¯‘APIç›¸å…³å‡½æ•°
class TencentTranslator:
    """è…¾è®¯ç¿»è¯‘APIå°è£…ç±»"""
    
    def __init__(self, secret_id, secret_key, region='ap-beijing'):
        self.secret_id = secret_id
        self.secret_key = secret_key
        self.region = region
        
    def translate(self, text, source='en', target='zh'):
        """ç¿»è¯‘æ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = get_memory_cache_key(text, 'tencent', source, target)
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        cached_result = load_from_memory_cache(cache_key)
        if cached_result is not None:
            return cached_result
        
        try:
            from tencentcloud.common import credential
            from tencentcloud.common.exception.tencent_cloud_sdk_exception import TencentCloudSDKException
            from tencentcloud.tmt.v20180321 import tmt_client, models
            
            # å®ä¾‹åŒ–ä¸€ä¸ªè®¤è¯å¯¹è±¡
            cred = credential.Credential(self.secret_id, self.secret_key)
            
            # å®ä¾‹åŒ–è¦è¯·æ±‚äº§å“çš„clientå¯¹è±¡
            client = tmt_client.TmtClient(cred, self.region)
            
            # å®ä¾‹åŒ–ä¸€ä¸ªè¯·æ±‚å¯¹è±¡
            req = models.TextTranslateRequest()
            
            # è®¾ç½®è¯·æ±‚å‚æ•°
            req.SourceText = text
            req.Source = source
            req.Target = target
            req.ProjectId = 0
            
            # é€šè¿‡clientå¯¹è±¡è°ƒç”¨æƒ³è¦è®¿é—®çš„æ¥å£
            resp = client.TextTranslate(req)
            
            # ä¿å­˜åˆ°ç¼“å­˜
            save_to_memory_cache(cache_key, resp.TargetText)
            
            # è¿”å›ç¿»è¯‘ç»“æœ
            return resp.TargetText
                
        except TencentCloudSDKException as err:
            raise Exception(f"è…¾è®¯ç¿»è¯‘APIè°ƒç”¨å¤±è´¥: {err}")
        except Exception as e:
            raise Exception(f"è…¾è®¯ç¿»è¯‘APIè°ƒç”¨å¤±è´¥: {str(e)}")

def create_translator(engine='google', secret_id=None, secret_key=None):
    """åˆ›å»ºç¿»è¯‘å™¨å®ä¾‹"""
    if engine == 'google':
        return CachedGoogleTranslator(source='en', target='zh-CN')
    elif engine == 'tencent':
        if not secret_id or not secret_key:
            raise ValueError("è…¾è®¯ç¿»è¯‘APIéœ€è¦æä¾›SecretIdå’ŒSecretKey")
        return TencentTranslator(secret_id, secret_key)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç¿»è¯‘å¼•æ“: {engine}")

class CachedGoogleTranslator:
    """å¸¦ç¼“å­˜çš„Googleç¿»è¯‘å™¨"""
    
    def __init__(self, source='en', target='zh-CN'):
        from deep_translator import GoogleTranslator
        self.translator = GoogleTranslator(source=source, target=target)
        
    def translate(self, text, source='en', target='zh-CN'):
        """ç¿»è¯‘æ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ï¼‰"""
        # ç”Ÿæˆç¼“å­˜é”®
        cache_key = get_memory_cache_key(text, 'google', source, target)
        
        # å°è¯•ä»ç¼“å­˜åŠ è½½
        cached_result = load_from_memory_cache(cache_key)
        if cached_result is not None:
            return cached_result
        
        # è°ƒç”¨Googleç¿»è¯‘
        result = self.translator.translate(text)
        
        # ä¿å­˜åˆ°ç¼“å­˜
        save_to_memory_cache(cache_key, result)
        
        return result 

# ========== AIæ‰¹é‡æ ‡æ³¨ç¼“å­˜ä¸ç»Ÿä¸€è°ƒç”¨å·¥å…· ==========
import hashlib
AI_CACHE_DIR = "ai_label_cache"
os.makedirs(AI_CACHE_DIR, exist_ok=True)

def get_ai_cache_key(text, prompt, model):
    key = f"{text}_{prompt}_{model}"
    return hashlib.md5(key.encode('utf-8')).hexdigest()

def save_ai_label_to_cache(cache_key, label):
    with open(os.path.join(AI_CACHE_DIR, f"{cache_key}.pkl"), "wb") as f:
        pickle.dump(label, f)

def load_ai_label_from_cache(cache_key):
    path = os.path.join(AI_CACHE_DIR, f"{cache_key}.pkl")
    if os.path.exists(path):
        try:
            with open(path, "rb") as f:
                return pickle.load(f)
        except:
            return None
    return None

def call_ai_model(text, prompt, model, api_key, max_retries=3):
    for attempt in range(max_retries):
        try:
            # å¤„ç†promptæ ¼å¼åŒ–ï¼Œæ”¯æŒå¤šç§å ä½ç¬¦æ ¼å¼
            try:
                if "{text}" in prompt:
                    formatted_prompt = prompt.format(text=text)
                elif "{Content}" in prompt:
                    formatted_prompt = prompt.format(Content=text)
                else:
                    # å¦‚æœæ²¡æœ‰å ä½ç¬¦ï¼Œç›´æ¥ä½¿ç”¨prompt
                    formatted_prompt = prompt
            except Exception:
                # å¦‚æœæ ¼å¼åŒ–å¤±è´¥ï¼Œç›´æ¥ä½¿ç”¨prompt
                formatted_prompt = prompt
            
            if model == "OpenAI":
                import openai
                openai.api_key = api_key
                response = openai.ChatCompletion.create(
                    model="gpt-3.5-turbo",
                    messages=[{"role": "user", "content": formatted_prompt}],
                    temperature=0
                )
                return response['choices'][0]['message']['content'].strip()
            elif model == "Deepseek":
                import openai
                version = getattr(openai, '__version__', '0.0.0')
                if version.startswith('1.'):
                    # openai >=1.0.0
                    client = openai.OpenAI(
                        api_key=api_key,
                        base_url="https://api.deepseek.com/v1"
                    )
                    response = client.chat.completions.create(
                        model="deepseek-chat",
                        messages=[{"role": "user", "content": formatted_prompt}],
                        temperature=0,
                        max_tokens=1000
                    )
                    return response.choices[0].message.content.strip()
                else:
                    # openai 0.x å…¼å®¹å†™æ³•
                    openai.api_key = api_key
                    openai.api_base = "https://api.deepseek.com/v1"
                    response = openai.ChatCompletion.create(
                        model="deepseek-chat",
                        messages=[{"role": "user", "content": formatted_prompt}],
                        temperature=0
                    )
                    return response['choices'][0]['message']['content'].strip()
            elif model == "é˜¿é‡Œåƒé—®":
                import dashscope
                rsp = dashscope.Generation.call(
                    model="qwen-turbo",
                    api_key=api_key,
                    messages=[{"role": "user", "content": formatted_prompt}]
                )
                return rsp['output']['choices'][0]['message']['content'].strip()
            else:
                return "[ä¸æ”¯æŒçš„æ¨¡å‹]"
        except Exception as e:
            error_msg = str(e)
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯è¯Šæ–­
            if "404" in error_msg:
                if model == "Deepseek":
                    return f"[AIå¤±è´¥:Deepseek APIé…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥API Keyå’Œæ¨¡å‹åç§°ã€‚é”™è¯¯è¯¦æƒ…: {error_msg}]"
                elif model == "OpenAI":
                    return f"[AIå¤±è´¥:OpenAI APIé…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥API Keyã€‚é”™è¯¯è¯¦æƒ…: {error_msg}]"
                elif model == "é˜¿é‡Œåƒé—®":
                    return f"[AIå¤±è´¥:é˜¿é‡Œåƒé—®APIé…ç½®é”™è¯¯ï¼Œè¯·æ£€æŸ¥API Keyã€‚é”™è¯¯è¯¦æƒ…: {error_msg}]"
            elif "401" in error_msg:
                return f"[AIå¤±è´¥:API Keyæ— æ•ˆï¼Œè¯·æ£€æŸ¥API Keyæ˜¯å¦æ­£ç¡®ã€‚é”™è¯¯è¯¦æƒ…: {error_msg}]"
            elif "429" in error_msg:
                return f"[AIå¤±è´¥:APIè°ƒç”¨é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åé‡è¯•ã€‚é”™è¯¯è¯¦æƒ…: {error_msg}]"
            elif "timeout" in error_msg.lower():
                return f"[AIå¤±è´¥:è¯·æ±‚è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥ã€‚é”™è¯¯è¯¦æƒ…: {error_msg}]"
            else:
                return f"[AIå¤±è´¥:{error_msg}]"
            
            if attempt < max_retries - 1:
                time.sleep(2)
                continue 

# ========== å†…å­˜ç¼“å­˜ç³»ç»Ÿï¼ˆé€‚ç”¨äºç½‘é¡µéƒ¨ç½²ï¼‰ ==========
import threading
from collections import OrderedDict
import time

class MemoryCache:
    """å†…å­˜ç¼“å­˜ç³»ç»Ÿï¼Œé€‚ç”¨äºç½‘é¡µéƒ¨ç½²ç¯å¢ƒ"""
    
    def __init__(self, max_size=1000, ttl_hours=24):
        self.max_size = max_size
        self.ttl_seconds = ttl_hours * 3600
        self.cache = OrderedDict()
        self.lock = threading.Lock()
    
    def get(self, key):
        """è·å–ç¼“å­˜å€¼"""
        with self.lock:
            if key in self.cache:
                value, timestamp = self.cache[key]
                # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
                if time.time() - timestamp < self.ttl_seconds:
                    # æ›´æ–°è®¿é—®é¡ºåº
                    self.cache.move_to_end(key)
                    return value
                else:
                    # åˆ é™¤è¿‡æœŸé¡¹
                    del self.cache[key]
        return None
    
    def set(self, key, value):
        """è®¾ç½®ç¼“å­˜å€¼"""
        with self.lock:
            # å¦‚æœç¼“å­˜å·²æ»¡ï¼Œåˆ é™¤æœ€æ—§çš„é¡¹
            if len(self.cache) >= self.max_size:
                self.cache.popitem(last=False)
            
            self.cache[key] = (value, time.time())
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self.lock:
            self.cache.clear()
    
    def get_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        with self.lock:
            current_time = time.time()
            valid_items = 0
            expired_items = 0
            
            for key, (value, timestamp) in self.cache.items():
                if current_time - timestamp < self.ttl_seconds:
                    valid_items += 1
                else:
                    expired_items += 1
            
            return {
                'total_items': len(self.cache),
                'valid_items': valid_items,
                'expired_items': expired_items,
                'max_size': self.max_size,
                'ttl_hours': self.ttl_seconds / 3600
            }

# å…¨å±€å†…å­˜ç¼“å­˜å®ä¾‹
memory_cache = MemoryCache(max_size=2000, ttl_hours=24)

def get_memory_cache_key(text, engine, source='en', target='zh'):
    """ç”Ÿæˆå†…å­˜ç¼“å­˜é”®"""
    content = f"{text}_{engine}_{source}_{target}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def save_to_memory_cache(cache_key, translation):
    """ä¿å­˜ç¿»è¯‘ç»“æœåˆ°å†…å­˜ç¼“å­˜"""
    memory_cache.set(cache_key, translation)

def load_from_memory_cache(cache_key):
    """ä»å†…å­˜ç¼“å­˜åŠ è½½ç¿»è¯‘ç»“æœ"""
    return memory_cache.get(cache_key)

def get_memory_cache_stats():
    """è·å–å†…å­˜ç¼“å­˜ç»Ÿè®¡"""
    return memory_cache.get_stats()

def clear_memory_cache():
    """æ¸…ç©ºå†…å­˜ç¼“å­˜"""
    memory_cache.clear() 
